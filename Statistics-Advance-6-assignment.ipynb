{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d596fc5-9028-40b4-aa43-962fc0c5ebf7",
   "metadata": {},
   "source": [
    "#Q1\n",
    "Analysis of Variance (ANOVA) is a statistical technique used to compare means across two or more groups. It assesses whether there are statistically significant differences between the means of these groups while accounting for variability within and between the groups. ANOVA is based on several assumptions, and violating these assumptions can impact the validity of the results. The key assumptions for ANOVA are:\n",
    "\n",
    "Independence: The observations within each group should be independent of each other. This means that the values of one observation should not be influenced by the values of other observations within the same group.\n",
    "\n",
    "Normality: The distribution of the residuals (the differences between observed and predicted values) should be approximately normal within each group. This assumption is about the normality of the population distributions, not the sample distributions.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity): The variance of the residuals should be approximately equal across all groups. This means that the spread of data points around the mean should be consistent across groups.\n",
    "\n",
    "Homogeneity of Regression Slopes (Only for Two-Way ANOVA): This assumption is specific to two-way ANOVA and it requires that the relationship between the dependent variable and the independent variable(s) is consistent across different levels of the other independent variable.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results:\n",
    "\n",
    "Non-Independence: Violation of the independence assumption can occur in scenarios where there is a natural structure or order among observations, such as time series data or repeated measures on the same subjects. Ignoring this can lead to incorrect conclusions.\n",
    "\n",
    "Non-Normality: If the residuals do not follow a normal distribution within each group, the p-values and confidence intervals generated by ANOVA may be inaccurate. This can happen when the data is skewed or contains extreme outliers.\n",
    "\n",
    "Heteroscedasticity: Unequal variances among groups can affect the significance tests and confidence intervals. If the assumption is violated, it can lead to incorrect conclusions about the differences between group means.\n",
    "\n",
    "Interaction Effects (Two-Way ANOVA): If the relationship between the dependent variable and one independent variable changes depending on the level of another independent variable, the assumption of homogeneity of regression slopes is violated. This means that the impact of one independent variable is not consistent across the different levels of the other independent variable.\n",
    "\n",
    "When these assumptions are violated, the ANOVA results may be misleading or incorrect. In such cases, it might be necessary to explore alternative statistical techniques or consider transformations on the data to mitigate the violations. Additionally, non-parametric tests (which make fewer assumptions) can be used when the assumptions of ANOVA are seriously violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eccb9a-f91b-49c2-9125-d67004d182e7",
   "metadata": {},
   "source": [
    "#Q2\n",
    "There are three main types of Analysis of Variance (ANOVA): one-way ANOVA, two-way ANOVA, and repeated measures ANOVA. Each type is used in specific situations to analyze the variance between groups and determine if there are significant differences among group means. Here's an overview of each type and when they would be used:\n",
    "\n",
    "One-Way ANOVA:\n",
    "\n",
    "Number of Factors: One categorical independent variable (factor) with three or more levels or groups.\n",
    "Use Case: When you want to compare means across multiple independent groups. For example, you might use one-way ANOVA to analyze the effects of different teaching methods (e.g., lecture, discussion, hands-on) on student test scores.\n",
    "Two-Way ANOVA:\n",
    "\n",
    "Number of Factors: Two categorical independent variables (factors), often referred to as \"factor A\" and \"factor B,\" and their interactions.\n",
    "Use Case: When you want to investigate the combined effects of two independent variables on the dependent variable. For example, a two-way ANOVA could be used to examine how both gender and treatment type impact patient recovery time.\n",
    "Repeated Measures ANOVA:\n",
    "\n",
    "Number of Factors: One categorical independent variable (factor) with two or more related measures or time points.\n",
    "Use Case: When you have collected measurements from the same subjects or items at multiple time points or under multiple conditions. Repeated measures ANOVA is commonly used in longitudinal studies or when analyzing data with a within-subjects design. An example is studying the effects of a training program on individuals' performance measured before and after the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea501f61-fe41-4413-9ecc-a521f2172840",
   "metadata": {},
   "source": [
    "#Q3\n",
    "In analysis of variance (ANOVA), the partitioning of variance refers to the process of decomposing the total variance observed in a dataset into different components associated with different sources of variation. ANOVA is a statistical technique used to compare means among different groups and determine if there are statistically significant differences between those groups. The partitioning of variance is crucial in ANOVA as it helps us understand the relative contributions of various factors to the overall variability observed in the data.\n",
    "\n",
    "In a typical one-way ANOVA scenario, the total variance observed in the data is divided into two main components: the variance between groups and the variance within groups. The formula for total variance can be expressed as:\n",
    "\n",
    "Total Variance = Variance Between Groups + Variance Within Groups\n",
    "\n",
    "Variance Between Groups: This component represents the variability of the group means with respect to the overall mean. It indicates how much the group means differ from each other. If this component is large relative to the within-group variance, it suggests that there are significant differences between the group means.\n",
    "\n",
    "Variance Within Groups: This component represents the variability of the individual data points within each group around their respective group means. It captures the random variability within each group that is not attributed to the differences between the group means.\n",
    "\n",
    "By comparing the sizes of these two components, ANOVA helps us determine whether the observed differences between group means are statistically significant or if they could have occurred due to random chance. If the variance between groups is significantly larger than the variance within groups, it suggests that there are real differences between the groups' means.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "Interpretation of Group Differences: ANOVA allows us to assess whether the differences between groups are likely due to the effect of the independent variable or if they could be attributed to random variability.\n",
    "\n",
    "Hypothesis Testing: ANOVA provides a statistical framework for hypothesis testing regarding the equality of means across groups. It helps researchers determine if these differences are statistically significant.\n",
    "\n",
    "Effect Size Estimation: By quantifying the proportion of variance explained by the between-group differences, ANOVA provides a way to estimate the effect size of the independent variable.\n",
    "\n",
    "Experimental Design: Understanding how variance is partitioned can inform the design of future experiments. For instance, if most of the variability is within groups, it might suggest that the experimental conditions need to be refined to achieve clearer between-group differences.\n",
    "\n",
    "Generalization: A better understanding of variance components can lead to more robust and accurate generalizations about the population from which the sample was drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4a4487-d63d-4352-a4c1-278f729db6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of IRIS dataset : \n",
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "Values for Sepal Length vs Species:\n",
      "SSE: 63.2121\n",
      "SSR: 38.9562\n",
      "SST: 102.1683\n",
      "             df     sum_sq    mean_sq           F        PR(>F)\n",
      "species     2.0  63.212133  31.606067  119.264502  1.669669e-31\n",
      "Residual  147.0  38.956200   0.265008         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "df_iris = sns.load_dataset('iris')\n",
    "print('Top 5 rows of IRIS dataset : ')\n",
    "print(df_iris.head())\n",
    "\n",
    "model = ols('sepal_length ~ species', data=df_iris).fit()\n",
    "\n",
    "print('Values for Sepal Length vs Species:')\n",
    "SSE = model.ess\n",
    "SSR = model.ssr\n",
    "SST = SSE + SSR\n",
    "\n",
    "print('SSE:', round(SSE,4))\n",
    "print('SSR:', round(SSR,4))\n",
    "print('SST:', round(SST,4))\n",
    "\n",
    "print(anova_lm(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499a5070-0a8e-40fa-914e-1105cc957f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of Tooth Growth Dataset\n",
      "    len supp  dose\n",
      "0   4.2   VC   0.5\n",
      "1  11.5   VC   0.5\n",
      "2   7.3   VC   0.5\n",
      "3   5.8   VC   0.5\n",
      "4   6.4   VC   0.5\n",
      "Main effects:\n",
      "C(supp)     205.350000\n",
      "C(dose)    2426.434333\n",
      "Name: sum_sq, dtype: float64\n",
      "Interaction effect:\n",
      "C(supp):C(dose)    108.319\n",
      "Name: sum_sq, dtype: float64\n",
      "ANOVA Table:\n",
      "                      sum_sq    df          F        PR(>F)\n",
      "C(supp)           205.350000   1.0  15.571979  2.311828e-04\n",
      "C(dose)          2426.434333   2.0  91.999965  4.046291e-18\n",
      "C(supp):C(dose)   108.319000   2.0   4.106991  2.186027e-02\n",
      "Residual          712.106000  54.0        NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = sm.datasets.get_rdataset(\"ToothGrowth\", \"datasets\").data\n",
    "\n",
    "print('Top 5 rows of Tooth Growth Dataset')\n",
    "print(data.head())\n",
    "\n",
    "model_formula = \"len ~ C(supp) + C(dose) + C(supp):C(dose)\"\n",
    "\n",
    "model = ols(model_formula, data).fit()\n",
    "\n",
    "main_effects = sm.stats.anova_lm(model, typ=2)['sum_sq'][:2]\n",
    "interaction_effect = sm.stats.anova_lm(model, typ=2)['sum_sq'][2:3]\n",
    "\n",
    "print(\"Main effects:\")\n",
    "print(main_effects)\n",
    "print(\"Interaction effect:\")\n",
    "print(interaction_effect)\n",
    "print(\"ANOVA Table:\")\n",
    "print(anova_lm(model,typ=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c31884-9519-4255-a97a-f022d4d8a7ca",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "In a one-way ANOVA, the F-statistic is used to test the null hypothesis that the means of the groups are equal against the alternative hypothesis that at least one group mean is different from the others. The p-value associated with the F-statistic indicates the probability of observing the data, or more extreme data, under the assumption that the null hypothesis is true. A small p-value suggests that the observed differences between group means are statistically significant, and you would reject the null hypothesis.\n",
    "\n",
    "Given your provided F-statistic of 5.23 and a p-value of 0.02, here's how you can interpret the results:\n",
    "\n",
    "F-Statistic (5.23): The F-statistic is a measure of the ratio of the variability between groups to the variability within groups. A larger F-statistic suggests that the variability between groups is larger compared to the variability within groups. In your case, a value of 5.23 indicates that there is some evidence that the group means are not all equal.\n",
    "\n",
    "P-Value (0.02): The p-value is the probability of obtaining results as extreme as the ones observed, assuming that the null hypothesis (equal group means) is true. A p-value of 0.02 means that if the null hypothesis were true, you would expect to see data as extreme as what you observed only about 2% of the time. This is a relatively low probability, suggesting that the observed differences between the groups' means are unlikely to have occurred by random chance alone.\n",
    "\n",
    "Conclusion: With a small p-value (0.02), you have evidence to reject the null hypothesis. This indicates that there are statistically significant differences between the groups. In other words, at least one group mean is likely to be different from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aca753-c2dd-46dc-8285-9f6ced9a98f8",
   "metadata": {},
   "source": [
    "#Q7\n",
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity and reliability of your results. Missing data can occur for various reasons, such as participant dropout, equipment malfunction, or incomplete responses. There are several methods to handle missing data, each with its own advantages and potential consequences. Here are some common methods and their potential consequences:\n",
    "\n",
    "Listwise Deletion (Complete Case Analysis): This method involves removing cases with missing data from the analysis. While it is straightforward, it can lead to reduced sample size, loss of statistical power, and potential bias if the missing data are not random.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Reduced sample size, which may decrease the power to detect effects.\n",
    "Potential bias if the missing data are related to the outcome or predictors, leading to non-representative results.\n",
    "Ignoring potentially valuable information if missingness is related to meaningful patterns.\n",
    "Pairwise Deletion (Available Case Analysis): This method uses available data for each specific analysis, so cases with missing data for specific variables are excluded only from the analyses involving those variables.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Inconsistency in sample size across analyses, which can complicate interpretation.\n",
    "Increased risk of Type I errors if the missing data are not missing completely at random.\n",
    "Mean Imputation: Missing values are replaced with the mean value of the non-missing data for that variable. This method can lead to an underestimation of standard errors and the loss of variability in the imputed variable.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Underestimation of standard errors, which can affect hypothesis tests and confidence intervals.\n",
    "Reduction of variance in the imputed variable, potentially affecting the ability to detect real effects.\n",
    "Last Observation Carried Forward (LOCF): Missing data are replaced with the last observed value for that participant. This method assumes that the participant's status remains unchanged from the last observed time point until the next observation.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Can lead to biased estimates if participants' statuses change over time.\n",
    "May not accurately represent the true trajectory of the variable.\n",
    "Multiple Imputation: This more sophisticated approach involves creating multiple imputed datasets based on the observed data and their relationships. The analyses are then performed on each imputed dataset, and results are combined to obtain estimates and standard errors.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Can provide more accurate estimates and valid statistical inference if assumptions about the missing data mechanism are met.\n",
    "Requires careful consideration of the imputation model and potential bias introduced if assumptions are violated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7d8b6-e7b6-4fbd-bb4e-77ac4428bab4",
   "metadata": {},
   "source": [
    "#Q8\n",
    "Post-hoc tests are used after conducting an ANOVA to determine which specific group differences are statistically significant when a significant main effect or interaction is detected. ANOVA can tell you that there are differences between groups, but it doesn't specify which groups are different from each other. Post-hoc tests help you identify these differences. Here are some common post-hoc tests and when to use each one:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD):\n",
    "\n",
    "Use when you have conducted a one-way ANOVA and you have three or more groups.\n",
    "It controls the familywise error rate, providing a good balance between Type I error control and statistical power.\n",
    "Appropriate for situations where you want to test all possible pairwise group comparisons.\n",
    "Example: You conducted an experiment to compare the effectiveness of three different teaching methods on student exam scores. The ANOVA showed a significant difference among the teaching methods. To identify which pairs of teaching methods are significantly different from each other, you can use Tukey's HSD.\n",
    "\n",
    "Bonferroni Correction:\n",
    "\n",
    "Use when conducting multiple pairwise comparisons after an ANOVA.\n",
    "It's a conservative approach that controls the familywise error rate by adjusting the significance level for each comparison.\n",
    "Appropriate when you want to maintain a strict control over the overall Type I error rate.\n",
    "Example: In the same teaching methods experiment, you are comparing all possible pairs of teaching methods. Since you're conducting several comparisons, you might choose to apply the Bonferroni correction to adjust the p-values to a more stringent level.\n",
    "\n",
    "Sidak Correction:\n",
    "\n",
    "Similar to the Bonferroni correction, it's used to control the familywise error rate for multiple comparisons.\n",
    "The Sidak correction can be less conservative than Bonferroni for larger numbers of comparisons.\n",
    "Example: If you have a large number of pairwise comparisons to make, such as in a genetic study with multiple variables, you might consider using the Sidak correction to control for the increased Type I error risk.\n",
    "\n",
    "Dunn's Test:\n",
    "\n",
    "Use when you have conducted a non-parametric ANOVA (e.g., Kruskal-Wallis test) and need to perform post-hoc pairwise comparisons.\n",
    "It's a non-parametric alternative to Tukey's HSD or Bonferroni corrections.\n",
    "Example: You conducted a Kruskal-Wallis test to compare the medians of multiple groups. Since this is a non-parametric test, you can use Dunn's test for pairwise comparisons to identify which groups have significantly different medians.\n",
    "\n",
    "Holm's Method:\n",
    "\n",
    "A stepwise procedure that adjusts p-values for multiple comparisons in a less conservative way compared to Bonferroni.\n",
    "It starts with the most significant p-value and adjusts it. If it remains significant, it proceeds to the next smallest p-value, adjusting it accordingly.\n",
    "Example: You conducted a two-way ANOVA and want to perform pairwise comparisons for both main effects and interaction effects. Holm's method can be useful to control the familywise error rate while exploring multiple comparisons.\n",
    "\n",
    "Post-hoc tests are important to avoid making false conclusions about group differences. However, it's important to choose a post-hoc test that is appropriate for your specific data and research questions, and to interpret the results with consideration of the chosen method's assumptions and adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bf3ca8-a9e8-41dd-9eef-aa57a786d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 57.06379442059458\n",
      "p-value: 4.5619061215783055e-19\n",
      "We reject the null hypothesis.\n",
      "Conclusion : The mean weight loss is different for at least one diet.\n"
     ]
    }
   ],
   "source": [
    "#Q9\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "np.random.seed(1)\n",
    "diet_A = np.random.normal(5, 1, 50)\n",
    "diet_B = np.random.normal(4, 1, 50)\n",
    "diet_C = np.random.normal(3, 1, 50)\n",
    "\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "null_hypothesis = \"The mean weight loss is the same for all three diets.\"\n",
    "alternate_hypothesis = \"The mean weight loss is different for at least one diet.\"\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis.\")\n",
    "    print(f\"Conclusion : {alternate_hypothesis}\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis.\")\n",
    "    print(f\"Conclusion : {null_hypothesis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3dcb11-6cf5-4e88-9dce-61e4bca3834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Data example :\n",
      "  Software Experience       Time\n",
      "0        A     Novice  12.828739\n",
      "1        A     Novice  16.994691\n",
      "2        A     Novice  15.565957\n",
      "3        A     Novice  11.987411\n",
      "4        A     Novice  13.842799\n",
      "                             df      sum_sq     mean_sq          F  \\\n",
      "C(Software)                 2.0  204.881181  102.440590  18.135666   \n",
      "C(Experience)               1.0  165.079097  165.079097  29.224933   \n",
      "C(Software):C(Experience)   2.0   17.481552    8.740776   1.547431   \n",
      "Residual                   56.0  316.319953    5.648571        NaN   \n",
      "\n",
      "                                 PR(>F)  \n",
      "C(Software)                8.460472e-07  \n",
      "C(Experience)              1.375177e-06  \n",
      "C(Software):C(Experience)  2.217544e-01  \n",
      "Residual                            NaN  \n",
      "\n",
      "\n",
      "Conclusion: There is a significant main effect of software.\n",
      "Conclusion: There is a significant main effect of experience.\n",
      "Conclusion: There is no significant interaction effect between software and experience.\n"
     ]
    }
   ],
   "source": [
    "#Q10\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "time_novice = np.random.normal(loc=15, scale=2, size=30)\n",
    "time_expert = np.random.normal(loc=10, scale=2, size=30)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Software': ['A']*20 + ['B']*20 + ['C']*20,\n",
    "    'Experience': ['Novice']*30 + ['Experienced']*30,\n",
    "    'Time': list(time_novice)+list(time_expert)\n",
    "})\n",
    "\n",
    "print('Simulated Data example :')\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=data).fit()\n",
    "table = sm.stats.anova_lm(model, typ=1)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "print(table)\n",
    "print('\\n')\n",
    "if table['PR(>F)'][0] < alpha:\n",
    "    print(\"Conclusion: There is a significant main effect of software.\")\n",
    "else:\n",
    "    print(\"Conclusion: There is no significant main effect of software.\")\n",
    "\n",
    "if table['PR(>F)'][1] < alpha:\n",
    "    print(\"Conclusion: There is a significant main effect of experience.\")\n",
    "else:\n",
    "    print(\"Conclusion: There is no significant main effect of experience.\")\n",
    "\n",
    "if table['PR(>F)'][2] < alpha:\n",
    "    print(\"Conclusion: There is a significant interaction effect between software and experience.\")\n",
    "else:\n",
    "    print(\"Conclusion: There is no significant interaction effect between software and experience.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0258415-f128-41c0-9ee6-d449982bd36c",
   "metadata": {},
   "source": [
    "Here are the interpretations of the three conclusions:\n",
    "\"There is a significant main effect of software\": This means that the software programs used by the employees have a significant impact on the outcome variable (e.g., completion time), independent of the experience level of the employees. This suggests that the choice of software program is an important factor that should be considered carefully when completing this task.\n",
    "\n",
    "\"There is a significant main effect of experience\": This means that the experience level of the employees has a significant impact on the outcome variable, independent of the software program used. Specifically, this suggests that experienced employees may complete the task faster than novices, or vice versa. This finding can be helpful for the company to identify the best employees for a given task and to provide appropriate training for new employees.\n",
    "\n",
    "\"There is NO significant interaction effect between software and experience\": This means that the effect of software on the outcome variable does not depend on the experience level of the employees, and vice versa. This suggests that the software programs perform similarly for both novices and experienced employees. This finding can be helpful for the company to decide which software program to use, as they do not need to consider the experience level of the employees when making the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b078bd-b3df-4213-bfe0-a6b258a2f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated data for test_scores:\n",
      "   test_score    group\n",
      "0   70.079124  control\n",
      "1   70.780965  control\n",
      "2   68.814563  control\n",
      "3   69.387097  control\n",
      "4   66.185102  control\n",
      "t-statistic: -28.5074, p-value: 3.096206271894725e-49\n",
      "\n",
      "\n",
      "Reject the Null Hypothesis\n",
      "Conclusion : There is SIGNIFICANT difference in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "#Q11\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "test_score_control = np.random.normal(loc=70, scale=3, size=50)\n",
    "test_score_experimental = np.random.normal(loc=85, scale=3, size=50)\n",
    "\n",
    "df = pd.DataFrame({'test_score':list(test_score_control)+list(test_score_experimental),\n",
    "                   'group':['control']*50 + ['experimental']*50})\n",
    "\n",
    "print('Simulated data for test_scores:')\n",
    "print(df.head())\n",
    "\n",
    "null_hypothesis = \"There is NO difference in test scores between the control and experimental groups.\"\n",
    "alt_hypothesis = \"There is SIGNIFICANT difference in test scores between the control and experimental groups.\"\n",
    "\n",
    "control_scores = df[df['group'] == 'control']['test_score']\n",
    "experimental_scores = df[df['group'] == 'experimental']['test_score']\n",
    "t_stat, p_val = ttest_ind(control_scores, experimental_scores, equal_var=True)\n",
    "print(f\"t-statistic: {t_stat:.4f}, p-value: {p_val}\")\n",
    "print('\\n')\n",
    "\n",
    "alpha = 0.05\n",
    "if p_val<alpha:\n",
    "    print('Reject the Null Hypothesis')\n",
    "    print(f'Conclusion : {alt_hypothesis}')\n",
    "else:\n",
    "    print('Failed to reject the Null Hypothesis')\n",
    "    print(f'Conclusion : {null_hypothesis}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5069bf-2eb3-49b6-bce8-9d6c4cab6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(456)\n",
    "\n",
    "# generate sales data for Store A, B, and C\n",
    "sales_a = np.random.normal(loc=1000, scale=100, size=(30,))\n",
    "sales_b = np.random.normal(loc=1050, scale=150, size=(30,))\n",
    "sales_c = np.random.normal(loc=800, scale=80, size=(30,))\n",
    "\n",
    "# create a DataFrame to store the sales data\n",
    "sales_df = pd.DataFrame({'Store A': sales_a, 'Store B': sales_b, 'Store C': sales_c})\n",
    "\n",
    "# reshape the DataFrame for repeated measures ANOVA\n",
    "sales_melted = pd.melt(sales_df.reset_index(), id_vars=['index'], value_vars=['Store A', 'Store B', 'Store C'])\n",
    "sales_melted.columns = ['Day', 'Store', 'Sales']\n",
    "\n",
    "print('Generated data top 5 rows : ')\n",
    "print(sales_melted.head())\n",
    "\n",
    "\n",
    "rm_anova = AnovaRM(sales_melted, 'Sales', 'Day', within=['Store'])\n",
    "rm_results = rm_anova.fit()\n",
    "print(rm_results)\n",
    "\n",
    "if rm_results.anova_table['Pr > F'][0] < 0.05:\n",
    "    print('Reject the Null Hypothesis : \\nConcusion: Atleast one of the group has different mean.\\n')\n",
    "    print('Tukey HSD posthoc test:')\n",
    "    tukey_results = pairwise_tukeyhsd(sales_melted['Sales'], sales_melted['Store'])\n",
    "    print(tukey_results)\n",
    "else:\n",
    "    print('NO significant difference between groups.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
